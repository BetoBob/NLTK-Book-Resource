{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [3. Processing Raw Text](https://www.nltk.org/book/ch03.html) - Exercise Solutions\n",
    "\n",
    "* [NLTK-Book-Resource Repository](https://github.com/BetoBob/NLTK-Book-Resource)\n",
    "* [NLTK-Book-Resource Table of Contents](https://github.com/BetoBob/NLTK-Book-Resource#table-of-contents)\n",
    "\n",
    "Run the cell below before running any other code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.\n",
    "\n",
    "☼ Define a string `s = 'colorless'`. Write a Python statement that changes this to \"colourless\" using only the slice and concatenation operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'colorless'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'colourless'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[:4] + \"u\" + s[4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.\n",
    "\n",
    "☼ We can use the slice notation to remove morphological endings on words. For example, `'dogs'[:-1]` removes the last character of dogs, leaving dog. Use slice notation to remove the affixes from these words (we've inserted a hyphen to indicate the affix boundary, but omit this from your strings): `dish-es, run-ning, nation-ality, un-do, pre-heat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dish'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"dishes\"[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"running\"[:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nation'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"nationality\"[:-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'un'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"undo\"[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pre'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"preheat\"[:-4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.\n",
    "\n",
    "☼ We saw how we can generate an `IndexError` by indexing beyond the end of a string. Is it possible to construct an index that goes too far to the left, before the start of the string?\n",
    "\n",
    "### Solution\n",
    "\n",
    "* you will receive an `IndexError` for positive indecies `>=` to the length of the string\n",
    "* you will recive an `IndexError` for negative indeciews `<` the negative length of the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"testing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'g'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-b035325af127>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "s[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[-7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-28d43f500622>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "s[-8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.\n",
    "\n",
    "☼ We can specify a \"step\" size for the slice. The following returns every second character within the slice: `monty[6:11:2]`. It also works in the reverse direction: `monty[10:5:-2]` Try these for yourself, then experiment with different step values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "monty = 'Monty Python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pytho'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monty[6:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pto'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monty[6:11:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ohtyP'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monty[10:5:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'otP'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monty[10:5:-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.\n",
    "\n",
    "☼ What happens if you ask the interpreter to evaluate `monty[::-1]`? Explain why this is a reasonable result.\n",
    "\n",
    "### Solution\n",
    "\n",
    "* this will reverse the entire string\n",
    "\n",
    "`[<start>:<end>:<increment>]`\n",
    "\n",
    "By default (i.e. no value given), the start of a slice is the beginning of a list / string and the end of a slice if the end of the the list / string. The -1 increment means the string will be reversed from end to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nohtyP ytnoM'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monty[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.\n",
    "\n",
    "☼ Describe the class of strings matched by the following regular expressions.\n",
    "\n",
    "1. `[a-zA-Z]+`\n",
    "2. `[A-Z][a-z]*`\n",
    "3. `p[aeiou]{,2}t`\n",
    "4. `\\d+(\\.\\d+)?`\n",
    "5. `([^aeiou][aeiou][^aeiou])*`\n",
    "6. `\\w+|[^\\w\\s]+`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "\n",
    "1. All strings that:\n",
    "    * have at least one character\n",
    "    * use only alphabetical letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# 1 - 3\n",
    "wordlist = [w for w in nltk.corpus.words.words('en')]\n",
    "\n",
    "# 4 - 6\n",
    "chat_words = sorted(set(w for w in nltk.corpus.nps_chat.words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['codfishery',\n",
       " 'Melastomaceae',\n",
       " 'stillhouse',\n",
       " 'complot',\n",
       " 'Anthophora',\n",
       " 'Loiseleuria',\n",
       " 'bloodsucker',\n",
       " 'winberry',\n",
       " 'bespoke',\n",
       " 'wobbliness',\n",
       " 'ported',\n",
       " 'condylopod',\n",
       " 'torotoro',\n",
       " 'maam',\n",
       " 'diamide',\n",
       " 'Lodur',\n",
       " 'Kizil',\n",
       " 'prizetaker',\n",
       " 'introvolution',\n",
       " 'intersex']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1\n",
    "random.choices([w for w in wordlist if re.search('[a-zA-Z]+', w)], k=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. All strings that:\n",
    "    * start with a capital letter\n",
    "    * followed by all lower case alphabetical letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Papist',\n",
       " 'Predentata',\n",
       " 'Zostera',\n",
       " 'Duchess',\n",
       " 'Tuckahoe',\n",
       " 'Asterales',\n",
       " 'Derby',\n",
       " 'Musalmani',\n",
       " 'Lionel',\n",
       " 'Mendelize',\n",
       " 'Ponera',\n",
       " 'Digitaria',\n",
       " 'Forficulidae',\n",
       " 'Protoceratops',\n",
       " 'Hevea',\n",
       " 'Brabejum',\n",
       " 'Spirochaetaceae',\n",
       " 'Leviticalism',\n",
       " 'Brummagem',\n",
       " 'Tulalip']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2\n",
    "random.choices([w for w in wordlist if re.search('[A-Z][a-z]*', w)], k=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. All strings that:\n",
    "    * start with the letter 'p'\n",
    "    * followed by 0 - 2 vowels\n",
    "    * ending in 't'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pterothorax',\n",
       " 'Ephemeroptera',\n",
       " 'reptilian',\n",
       " 'crampet',\n",
       " 'captivator',\n",
       " 'Septemberism',\n",
       " 'prodespotic',\n",
       " 'apperceptionistic',\n",
       " 'metropathia',\n",
       " 'bypath',\n",
       " 'spatular',\n",
       " 'plecopterous',\n",
       " 'stript',\n",
       " 'angiopathy',\n",
       " 'aerotherapeutics',\n",
       " 'peripety',\n",
       " 'coleopteroid',\n",
       " 'amputative',\n",
       " 'uncaptivated',\n",
       " 'consumption']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3\n",
    "random.choices([w for w in wordlist if re.search('p[aeiou]{,2}t', w)], k=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above example are words that contain the regular expression as a substring. To find all words in this example that start with 'p' and end with 't', add the carrot `^` and dollar `$` symbol respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pat',\n",
       " 'paut',\n",
       " 'peat',\n",
       " 'pet',\n",
       " 'piet',\n",
       " 'pit',\n",
       " 'poet',\n",
       " 'poot',\n",
       " 'pot',\n",
       " 'pout',\n",
       " 'put']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 (continued)\n",
    "[w for w in wordlist if re.search('^p[aeiou]{,2}t$', w)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. All strings that:\n",
    "    * start with one or more digits\n",
    "    * optionally contain **one** period followed by more digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['28147',\n",
       " '92780',\n",
       " '20',\n",
       " '300',\n",
       " '64.8',\n",
       " '2006',\n",
       " '7',\n",
       " '73042',\n",
       " '98.6',\n",
       " '49',\n",
       " '1996',\n",
       " '1.99',\n",
       " '99703',\n",
       " '1930',\n",
       " '295',\n",
       " '73042',\n",
       " '0',\n",
       " '73042',\n",
       " '39.3',\n",
       " '51']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4\n",
    "random.choices([w for w in chat_words if re.search('^\\d+(\\.\\d+)?$', w)], k=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. All string that contain a three letter substring with this pattern: `(non-vowel)(vowel)(non-vowel)`. This can occur *zero times* (an empty string), *one time* following the three letter structure, or *n-number of times* as long as the next three letters follow the same pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['suggested',\n",
       " 'New',\n",
       " 'bumber',\n",
       " 'dobson',\n",
       " 'tastes',\n",
       " 'bar',\n",
       " 'rey',\n",
       " 'messenger',\n",
       " 'san',\n",
       " 'killed',\n",
       " 'sec',\n",
       " 'center',\n",
       " 'Her',\n",
       " 'burned',\n",
       " 'heh',\n",
       " 'bumber',\n",
       " 'van',\n",
       " 'wantin',\n",
       " 'Dustin',\n",
       " 'waz']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5\n",
    "random.choices([w for w in chat_words if re.search('^([^aeiou][aeiou][^aeiou])*$', w)], k=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. All word characters or all non-word characters and non-space characters (ex: `'####'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lysol',\n",
       " 'choco',\n",
       " 'back',\n",
       " 'tonight',\n",
       " 'Looking',\n",
       " 'ground',\n",
       " 'min',\n",
       " 'sores',\n",
       " 'bio',\n",
       " 'covered',\n",
       " 'snowy',\n",
       " '32',\n",
       " 'ACTION',\n",
       " '####',\n",
       " 'pumpkins',\n",
       " 'U197',\n",
       " 'Kent',\n",
       " 'Horace',\n",
       " 'Jesus',\n",
       " 'beg']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6\n",
    "random.choices([w for w in chat_words if re.search('^\\w+|[^\\w\\s]+$', w)], k=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.\n",
    "\n",
    "☼ Write regular expressions to match the following classes of strings:\n",
    "\n",
    "1. A single determiner (assume that a, an, and the are the only determiners).\n",
    "2. An arithmetic expression using integers, addition, and multiplication, such as `2*3+8`.\n",
    "\n",
    "### Solution\n",
    "\n",
    "* use `\\b` (for boundry) to mark the beginning or end of a non-whitespace word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This {an} especially important test to {the} determiners of {a} sentence.\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "\n",
    "string = \"This an especially important test to the determiners of a sentence.\"\n",
    "\n",
    "nltk.re_show(r\"\\bthe\\b|\\ban\\b|\\ba\\b\", string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the solution below assumes an integer without operators like `+` and `*` are valid arithmetic expressions\n",
    "* `-` and `\\` are excluded, but can be easily added in the `(\\+|\\*)` portion of the regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2 + 2} is {4} - {1} that's {3} quick maths {2*3+8}\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "\n",
    "string = \"2 + 2 is 4 - 1 that's 3 quick maths 2*3+8\"\n",
    "\n",
    "nltk.re_show(r\"\\d(\\W*(\\+|\\*)\\W*\\d)*\", string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.\n",
    "\n",
    "☼ Write a utility function that takes a URL as its argument, and returns the contents of the URL, with all HTML markup removed. Use `from urllib import request` and then `request.urlopen('http://nltk.org/').read().decode('utf8')` to access the contents of the URL.\n",
    "\n",
    "### Solution\n",
    "\n",
    "* use the `Beautiful Soup` library to easily get the text content of a web page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def remove_markup(URL):\n",
    "    page = request.urlopen(URL).read().decode('utf8')\n",
    "    soup = BeautifulSoup(page)\n",
    "    return soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Natural Language Toolkit — NLTK 3.5 documentation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NLTK 3.5 documentation\n",
      "\n",
      "next |\n",
      "          modules |\n",
      "          index\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Natural Language Toolkit¶\n",
      "NLTK is a leading platform for building Python programs to work with human language data.\n",
      "It provides easy-to-use interfaces to over 50 corpora and lexical\n",
      "resources such as WordNet,\n",
      "along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning,\n",
      "wrappers for industrial-strength NLP libraries,\n",
      "and an active discussion forum.\n",
      "Thanks to a hands-on guide introducing programming fundamentals alongside topics in computational linguistics, plus comprehensive API documentation,\n",
      "NLTK is suitable for linguists, engineers, students, educators, researchers, and industry users alike.\n",
      "NLTK is available for Windows, Mac OS X, and Linux. Best of all, NLTK is a free, open source, community-driven project.\n",
      "NLTK has been called “a wonderful tool for teaching, and working in, computational linguistics using Python,”\n",
      "and “an amazing library to play with natural language.”\n",
      "Natural Language Processing with Python provides a practical\n",
      "introduction to programming for language processing.\n",
      "Written by the creators of NLTK, it guides the reader through the fundamentals\n",
      "of writing Python programs, working with corpora, categorizing text, analyzing linguistic structure,\n",
      "and more.\n",
      "The online version of the book has been been updated for Python 3 and NLTK 3.\n",
      "(The original Python 2 version is still available at http://nltk.org/book_1ed.)\n",
      "\n",
      "Some simple things you can do with NLTK¶\n",
      "Tokenize and tag some text:\n",
      ">>> import nltk\n",
      ">>> sentence = \"\"\"At eight o'clock on Thursday morning\n",
      "... Arthur didn't feel very good.\"\"\"\n",
      ">>> tokens = nltk.word_tokenize(sentence)\n",
      ">>> tokens\n",
      "['At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning',\n",
      "'Arthur', 'did', \"n't\", 'feel', 'very', 'good', '.']\n",
      ">>> tagged = nltk.pos_tag(tokens)\n",
      ">>> tagged[0:6]\n",
      "[('At', 'IN'), ('eight', 'CD'), (\"o'clock\", 'JJ'), ('on', 'IN'),\n",
      "('Thursday', 'NNP'), ('morning', 'NN')]\n",
      "\n",
      "\n",
      "Identify named entities:\n",
      ">>> entities = nltk.chunk.ne_chunk(tagged)\n",
      ">>> entities\n",
      "Tree('S', [('At', 'IN'), ('eight', 'CD'), (\"o'clock\", 'JJ'),\n",
      "           ('on', 'IN'), ('Thursday', 'NNP'), ('morning', 'NN'),\n",
      "       Tree('PERSON', [('Arthur', 'NNP')]),\n",
      "           ('did', 'VBD'), (\"n't\", 'RB'), ('feel', 'VB'),\n",
      "           ('very', 'RB'), ('good', 'JJ'), ('.', '.')])\n",
      "\n",
      "\n",
      "Display a parse tree:\n",
      ">>> from nltk.corpus import treebank\n",
      ">>> t = treebank.parsed_sents('wsj_0001.mrg')[0]\n",
      ">>> t.draw()\n",
      "\n",
      "\n",
      "\n",
      "NB. If you publish work that uses NLTK, please cite the NLTK book as\n",
      "follows:\n",
      "\n",
      "Bird, Steven, Edward Loper and Ewan Klein (2009), Natural Language Processing with Python.  O’Reilly Media Inc.\n",
      "\n",
      "\n",
      "\n",
      "Next Steps¶\n",
      "\n",
      "sign up for release announcements\n",
      "join in the discussion\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents¶\n",
      "\n",
      "\n",
      "NLTK News\n",
      "Installing NLTK\n",
      "Installing NLTK Data\n",
      "Contribute to NLTK\n",
      "FAQ\n",
      "Wiki\n",
      "API\n",
      "HOWTO\n",
      "\n",
      "\n",
      "\n",
      "Index\n",
      "Module Index\n",
      "Search Page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Table of Contents\n",
      "\n",
      "NLTK News\n",
      "Installing NLTK\n",
      "Installing NLTK Data\n",
      "Contribute to NLTK\n",
      "FAQ\n",
      "Wiki\n",
      "API\n",
      "HOWTO\n",
      "\n",
      "\n",
      "Search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "next |\n",
      "            modules |\n",
      "            index\n",
      "\n",
      "\n",
      "\n",
      "Show Source\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        © Copyright 2020, NLTK Project.\n",
      "      Last updated on Apr 13, 2020.\n",
      "      Created using Sphinx 2.4.4.\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(remove_markup('http://nltk.org/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.\n",
    "\n",
    "☼ Save some text into a file `corpus.txt`. Define a function `load(f)` that reads from the file named in its sole argument, and returns a string containing the text of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(f):\n",
    "    f = open(f)\n",
    "    raw = f.read()\n",
    "    f.close()\n",
    "    \n",
    "    return raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a txt file :)\n",
      "It has a new line character. This looks like a '\\n' and creates a new line in the file.\n",
      "That's all. Thanks for reading!\n"
     ]
    }
   ],
   "source": [
    "print(load(\"data/example.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. \n",
    "\n",
    "☼ Rewrite the following loop as a list comprehension:\n",
    "\n",
    "```python\n",
    "sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']\n",
    "result = []\n",
    "for word in sent:\n",
    "    word_len = (word, len(word))\n",
    "    result.append(word_len)\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "`[('The', 3), ('dog', 3), ('gave', 4), ('John', 4), ('the', 3), ('newspaper', 9)]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [(s, len(s)) for s in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 3),\n",
       " ('dog', 3),\n",
       " ('gave', 4),\n",
       " ('John', 4),\n",
       " ('the', 3),\n",
       " ('newspaper', 9)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.\n",
    "\n",
    "☼ Define a string `raw` containing a sentence of your own choosing. Now, split raw on some character other than space, such as `'s'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = \"a string that splits on s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a ', 'tring that ', 'plit', ' on ', '']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.split('s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.\n",
    "\n",
    "☼ Write a for loop to print out the characters of a string, one per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"for loop magic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f\n",
      "o\n",
      "r\n",
      " \n",
      "l\n",
      "o\n",
      "o\n",
      "p\n",
      " \n",
      "m\n",
      "a\n",
      "g\n",
      "i\n",
      "c\n"
     ]
    }
   ],
   "source": [
    "for c in s:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.\n",
    "\n",
    "☼ What is the difference between calling split on a string with no argument or with `' '` as the argument, e.g. `sent.split()` versus `sent.split(' ')`? What happens when the string being split contains tab characters, consecutive space characters, or a sequence of tabs and spaces? \n",
    "\n",
    "**Tip:** Type `\\t` to write a tab character into a string\n",
    "\n",
    "### Solution\n",
    "* by default, the `.split()` function splits on  **all** whitespace characters. This includes the characters\n",
    "    * ' '\n",
    "    * '\\n'\n",
    "    * '\\t'\n",
    "    * and others\n",
    "* if you want to specify a string to split on, enter the string as an argument to the split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"A string to be split!\\n\\t With a tab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'string', 'to', 'be', 'split!', 'With', 'a', 'tab']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# notice the split function splits on \\n and \\t by default as well\n",
    "sent.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'string', 'to', 'be', 'split!\\n\\t', 'With', 'a', 'tab']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A string to be split!\\n', ' With a tab']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent.split('\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.\n",
    "\n",
    "Create a variable `words` containing a list of words. Experiment with `words.sort()` and `sorted(words)`. What is the difference?\n",
    "\n",
    "### Solution\n",
    "\n",
    "* `words.sort()` changes the order of the `words` list permanently\n",
    "* `sorted(words)` returns a new list that is sorted, but doesn't effect the old `words` list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "words1 = [\"these\", \"are\", \"a\", \"list\", \"of\", \"words\"]\n",
    "words2 = [\"these\", \"are\", \"a\", \"list\", \"of\", \"words\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words1.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'are', 'list', 'of', 'these', 'words']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'are', 'list', 'of', 'these', 'words']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(words2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['these', 'are', 'a', 'list', 'of', 'words']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.\n",
    "\n",
    "☼ What happens when the formatting strings `%6s` and `%-6s` are used to display strings that are longer than six characters?\n",
    "\n",
    "**Note:**\n",
    "\n",
    "* this question uses the **old** python format\n",
    "* the new python format would be `{:>6}` and `{:6}` respectively\n",
    "\n",
    "* [more information on old vs. new format](https://pyformat.info/)\n",
    "\n",
    "### Solution\n",
    "\n",
    "The full string is printed with no additional whitespace in each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mississippi\n"
     ]
    }
   ],
   "source": [
    "long_word = \"mississippi\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mississippi\n"
     ]
    }
   ],
   "source": [
    "print('%6s' % (long_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mississippi\n"
     ]
    }
   ],
   "source": [
    "print('%-6s' % (long_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mississippi\n"
     ]
    }
   ],
   "source": [
    "print(\"{:>6}\".format(long_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mississippi\n"
     ]
    }
   ],
   "source": [
    "print(\"{:6}\".format(long_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.\n",
    "\n",
    "◑ Read in some text from a corpus, tokenize it, and print the list of all *wh*-word types that occur. (*wh*-words in English are used in questions, relative clauses and exclamations: *who*, *which*, *what*, and so on.) Print them in order. Are any words duplicated in this list, because of the presence of case distinctions or punctuation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "* some 'wh' words start capitalized while others don't\n",
    "* some words are a combination of multiple words like *wholesale* and *wholeheartedly*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "sotu_tokens = nltk.corpus.state_union.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['who',\n",
       " 'wholesale',\n",
       " 'where',\n",
       " 'who',\n",
       " 'who',\n",
       " 'who',\n",
       " 'which',\n",
       " 'While',\n",
       " 'when',\n",
       " 'who',\n",
       " 'which',\n",
       " 'While',\n",
       " 'which',\n",
       " 'which',\n",
       " 'which',\n",
       " 'wholeheartedly',\n",
       " 'which',\n",
       " 'whole',\n",
       " 'where',\n",
       " 'which']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[s for s in sotu_tokens if s.lower().startswith('wh')][100:120]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. \n",
    "\n",
    "◑ Create a file consisting of words and (made up) frequencies, where each line consists of a word, the space character, and a positive integer, e.g. `fuzzy 53`. Read the file into a Python list using `open(filename).readlines()`. Next, break each line into its two fields using `split()`, and convert the number into an integer using `int()`. The result should be a list of the form: `[['fuzzy', 53], ...]`.\n",
    "\n",
    "* **Note:** It is a safer programming practice to open and close a document like this:\n",
    "\n",
    "```python\n",
    "f = open(filename)\n",
    "raw = f.readlines()\n",
    "f.close()\n",
    "```\n",
    "\n",
    "### Solution\n",
    "\n",
    "* a solution file is provided in `data/sol/freqs.txt`\n",
    "* after using `readlines()`, use a list comprehension and `split()` method to create the new list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['fuzzy', 53],\n",
       " ['cat', 9],\n",
       " ['triangle', 3],\n",
       " ['square', 4],\n",
       " ['octopus', 8],\n",
       " ['bob', 808]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = 'data/sol/freqs.txt'\n",
    "\n",
    "f = open(file)\n",
    "raw = f.readlines()\n",
    "f.close()\n",
    "\n",
    "[[w.split()[0], int(w.split()[1])] for w in raw]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20.\n",
    "\n",
    "◑ Write code to access a favorite webpage and extract some text from it. For example, access a weather site and extract the forecast top temperature for your town or city today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21.\n",
    "\n",
    "◑ Write a function `unknown()` that takes a URL as its argument, and returns a list of unknown words that occur on that webpage. In order to do this, extract all substrings consisting of lowercase letters (using re.findall()) and remove any items from this set that occur in the Words Corpus (nltk.corpus.words). Try to categorize these words manually and discuss your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22.\n",
    "\n",
    "◑ Examine the results of processing the URL http://news.bbc.co.uk/ using the regular expressions suggested above. You will see that there is still a fair amount of non-textual data there, particularly Javascript commands. You may also find that sentence breaks have not been properly preserved. Define further regular expressions that improve the extraction of text from this web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 23.\n",
    "\n",
    "◑ Are you able to write a regular expression to tokenize text in such a way that the word *don't* is tokenized into *do* and *n't*? Explain why this regular expression won't work: `«n't|\\w+»`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "This regular expression by default is *greedy*, so it will capture the `don` in `don't` first before it captures `n't`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{I} {don}'{t} {think} {this} {will} {work}.\n"
     ]
    }
   ],
   "source": [
    "string = \"I don't think this will work.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'don', 't', 'think', 'this', 'will', 'work']"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# naive solution\n",
    "re.findall(r\"n't|\\w+\", string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`n't` can be captured by checking for two substrings:\n",
    "1. a word using a non-greedy search method: `(\\w+?)\n",
    "2. the *optional* ending letters `n't`: `(n't)?`\n",
    "\n",
    "Because a non-greedy approach will be used, a `\\b` must be used to signify the end of a word or else the regular expression will capture the minimum amount of characters needed to form a word. This would mean that the regular expression would only chapture single characters instead of whole words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', ''),\n",
       " ('do', \"n't\"),\n",
       " ('think', ''),\n",
       " ('this', ''),\n",
       " ('will', ''),\n",
       " ('work', '')]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = re.findall(r\"(\\w+?)(n't)?\\b\", string)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `re.findall` method will return a list of pairs (tuples). To convert it into a list of strings, we can unpack the list of tuples using a list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'do', \"n't\", 'think', 'this', 'will', 'work']"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token for tup in results for token in tup if token] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 24.\n",
    "\n",
    "◑ Try to write code to convert text into *hAck3r*, using regular expressions and substitution, where `e → 3`, `i → 1`, `o → 0`, `l → |`, `s → 5`, `. → 5w33t!`, `ate → 8`. Normalize the text to lowercase before converting it. Add more substitutions of your own. Now try to map s to two different values: `$` for word-initial `s`, and `5` for word-internal `s`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25.\n",
    "\n",
    "◑ *Pig Latin* is a simple transformation of English text. Each word of the text is converted as follows: move any consonant (or consonant cluster) that appears at the start of the word to the end, then append *ay*, e.g. *string → ingstray*, *idle → idleay*. http://en.wikipedia.org/wiki/Pig_Latin\n",
    "\n",
    "1. Write a function to convert a word to Pig Latin.\n",
    "2. Write code that converts text, instead of individual words.\n",
    "3. Extend it further to preserve capitalization, to keep `qu` together (i.e. so that `quiet` becomes `ietquay`), and to detect when y is used as a consonant (e.g. `yellow`) vs a vowel (e.g. `style`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 26. \n",
    "\n",
    "◑ Download some text from a language that has vowel harmony (e.g. Hungarian), extract the vowel sequences of words, and create a vowel bigram table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 27.\n",
    "\n",
    "Python's `random` module includes a function `choice()` which randomly chooses an item from a sequence, e.g. choice(\"aehh \") will produce one of four possible characters, with the letter h being twice as frequent as the others. Write a generator expression that produces a sequence of 500 randomly chosen letters drawn from the string `\"aehh \"`, and put this expression inside a call to the `''.join()` function, to concatenate them into one long string. You should get a result that looks like uncontrolled sneezing or maniacal laughter: `he  haha ee  heheeh eha`. Use `split()` and `join()` again to normalize the whitespace in this string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.choice(\"aehh \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 28.\n",
    "\n",
    "◑ Consider the numeric expressions in the following sentence from the MedLine Corpus: *The corresponding free cortisol fractions in these sera were 4.53 +/- 0.15% and 8.16 +/- 0.23%, respectively.* Should we say that the numeric expression *4.53 +/- 0.15%* is three words? Or should we say that it's a single compound word? Or should we say that it is actually *nine* words, since it's read \"four point five three, plus or minus zero point fifteen percent\"? Or should we say that it's not a \"real\" word at all, since it wouldn't appear in any dictionary? Discuss these different possibilities. Can you think of application domains that motivate at least two of these answers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 29.\n",
    "\n",
    "◑ Readability measures are used to score the reading difficulty of a text, for the purposes of selecting texts of appropriate difficulty for language learners. Let us define μ<sub>w</sub> to be the average number of letters per word, and μ<sub>s</sub> to be the average number of words per sentence, in a given text. The Automated Readability Index (ARI) of the text is defined to be: 4.71 μ<sub>w</sub> + 0.5 μ<sub>s</sub> - 21.43. Compute the ARI score for various sections of the Brown Corpus, including section `f` (lore) and `j` (learned). Make use of the fact that `nltk.corpus.brown.words()` produces a sequence of words, while `nltk.corpus.brown.sents()` produces a sequence of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 30.\n",
    "\n",
    "◑ Use the Porter Stemmer to normalize some tokenized text, calling the stemmer on each word. Do the same thing with the Lancaster Stemmer and see if you observe any differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "* the third movie review in the `movie_reviews` corpused will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "tokens = movie_reviews.words(movie_reviews.fileids()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_list = [porter.stem(t) for t in tokens][0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lancaster_list = [lancaster.stem(t) for t in tokens][0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movi vs. movy\n",
      "like vs. lik\n",
      "these vs. thes\n",
      "make vs. mak\n",
      "jade vs. jad\n",
      "movi vs. movy\n",
      "viewer vs. view\n",
      "invent vs. inv\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    if porter_list[i] != lancaster_list[i]:\n",
    "        print(porter_list[i], \"vs.\", lancaster_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 31.\n",
    "\n",
    "◑ Define the variable saying to contain the list `['After', 'all', 'is', 'said', 'and', 'done', ',', 'more',\n",
    "'is', 'said', 'than', 'done', '.']`. Process this list using a `for` loop, and store the length of each word in a new list `lengths`. \n",
    "\n",
    "* **Hint:** begin by assigning the empty list to lengths, using `lengths = []`. Then each time through the loop, use `append()` to add another length value to the list. \n",
    "\n",
    "Now do the same thing using a list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 3, 2, 4, 3, 4, 1, 4, 2, 4, 4, 4, 1]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp = ['After', 'all', 'is', 'said', 'and', 'done', ',', 'more', 'is', 'said', 'than', 'done', '.']\n",
    "\n",
    "lengths = []\n",
    "for word in exp:\n",
    "    lengths.append(len(word))\n",
    "    \n",
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 3, 2, 4, 3, 4, 1, 4, 2, 4, 4, 4, 1]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(word) for word in exp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 32.\n",
    "\n",
    "◑ Define a variable `silly` to contain the string: `'newly formed bland ideas are inexpressible in an infuriating way'`. (This happens to be the legitimate interpretation that bilingual English-Spanish speakers can assign to Chomsky's famous nonsense phrase, colorless green ideas sleep furiously according to Wikipedia). Now write code to perform the following tasks:\n",
    "\n",
    "\n",
    "1. Split `silly` into a list of strings, one per word, using Python's `split()` operation, and save this to a variable called `bland`.\n",
    "2. Extract the second letter of each word in `silly` and join them into a string, to get `'eoldrnnnna'`.\n",
    "3. Combine the words in bland back into a single string, using join(). Make sure the words in the resulting string are separated with whitespace.\n",
    "4. Print the words of silly in alphabetical order, one per line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "silly = 'newly formed bland ideas are inexpressible in an infuriating way'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "bland = silly.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eoldrnnnna'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2\n",
    "''.join([word[1] for word in bland])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'newly formed bland ideas are inexpressible in an infuriating way'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3 \n",
    "' '.join(bland)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an\n",
      "are\n",
      "bland\n",
      "formed\n",
      "ideas\n",
      "in\n",
      "inexpressible\n",
      "infuriating\n",
      "newly\n",
      "way\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "for word in sorted(bland):\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 33.\n",
    "\n",
    "◑ The `index()` function can be used to look up items in sequences. For example, `'inexpressible'.index('e')` tells us the index of the first position of the letter e.\n",
    "\n",
    "\n",
    "1. What happens when you look up a substring, e.g. `'inexpressible'.index('re')`?\n",
    "2. Define a variable words containing a list of words. Now use `words.index()` to look up the position of an individual word.\n",
    "3. Define a variable `silly` as in the exercise above. Use the `index()` function in combination with list slicing to build a list phrase consisting of all the words up to (but not including) `in` in `silly`.\n",
    "\n",
    "### Soltuion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'inexpressible'.index('e')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking up a substring, the `.index` function finds the first occurance of that substrings and returns the index of the beginning character of the substring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1\n",
    "'inexpressible'.index('re')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1 (continued)\n",
    "'rly inexpressible'.index('re')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2\n",
    "words = \"these are a bunch of words\"\n",
    "words.index(\"are\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'newly formed bland ideas are '"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3\n",
    "silly = 'newly formed bland ideas are inexpressible in an infuriating way'\n",
    "\n",
    "silly[:silly.index('in')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 34.\n",
    "\n",
    "◑ Write code to convert nationality adjectives like Canadian and Australian to their corresponding nouns Canada and Australia (see http://en.wikipedia.org/wiki/List_of_adjectival_forms_of_place_names)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 35.\n",
    "\n",
    "◑ Read the LanguageLog post on phrases of the form as best *as p can and as best p can*, where *p* is a pronoun. Investigate this phenomenon with the help of a corpus and the `findall()` method for searching tokenized text described in [3.5](https://www.nltk.org/book/ch03.html#sec-useful-applications-of-regular-expressions). http://itre.cis.upenn.edu/~myl/languagelog/archives/002733.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 36.\n",
    "\n",
    "◑ Study the *lolcat* version of the book of Genesis, accessible as `nltk.corpus.genesis.words('lolcat.txt')`, and the rules for converting text into *lolspeak* at http://www.lolcatbible.com/index.php?title=How_to_speak_lolcat. Define regular expressions to convert English words into corresponding lolspeak words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 37.\n",
    "\n",
    "◑ Read about the `re.sub()` function for string substitution using regular expressions, using `help(re.sub)` and by consulting the further readings for this chapter. Use `re.sub` in writing code to remove HTML tags from an HTML file, and to normalize whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 38.\n",
    "\n",
    "★ An interesting challenge for tokenization is words that have been split across a line-break. E.g. if *long-term* is split, then we have the string `long-\\nterm`.\n",
    "\n",
    "1. Write a regular expression that identifies words that are hyphenated at a line-break. The expression will need to include the `\\n` character.\n",
    "2. Use `re.sub()` to remove the `\\n` character from these words.\n",
    "3. How might you identify words that should not remain hyphenated once the newline is removed, e.g. `'encyclo-\\npedia'`?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 39.\n",
    "\n",
    "★ Read the Wikipedia entry on [*Soundex*](https://en.wikipedia.org/wiki/Soundex). Implement this algorithm in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 40.\n",
    "\n",
    "★ Obtain raw texts from two or more genres and compute their respective reading difficulty scores as in the earlier exercise on reading difficulty. E.g. compare ABC Rural News and ABC Science News (`nltk.corpus.abc`). Use Punkt to perform sentence segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 41.\n",
    "\n",
    "★ Rewrite the following nested loop as a nested list comprehension:\n",
    "\n",
    "```python\n",
    "words = ['attribution', 'confabulation', 'elocution', 'sequoia', 'tenacious', 'unidirectional']\n",
    "\n",
    "vsequences = set()\n",
    "for word in words:\n",
    "    vowels = []\n",
    "    for char in word:\n",
    "        if char in 'aeiou':\n",
    "            vowels.append(char)\n",
    "    vsequences.add(''.join(vowels))\n",
    "\n",
    "sorted(vsequences)\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "`['aiuio', 'eaiou', 'eouio', 'euoia', 'oauaio', 'uiieioa']`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "\n",
    "* this code is returning a set of the vowels in each word\n",
    "* this solution will be done in parts\n",
    "\n",
    "**1.** Create a list of the words with a list comprehension. This will be our starting point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['attribution',\n",
       " 'confabulation',\n",
       " 'elocution',\n",
       " 'sequoia',\n",
       " 'tenacious',\n",
       " 'unidirectional']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** Create a list of letters for each word. This will require us to use a nested list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a', 't', 't', 'r', 'i', 'b', 'u', 't', 'i', 'o', 'n'],\n",
       " ['c', 'o', 'n', 'f', 'a', 'b', 'u', 'l', 'a', 't', 'i', 'o', 'n'],\n",
       " ['e', 'l', 'o', 'c', 'u', 't', 'i', 'o', 'n'],\n",
       " ['s', 'e', 'q', 'u', 'o', 'i', 'a'],\n",
       " ['t', 'e', 'n', 'a', 'c', 'i', 'o', 'u', 's'],\n",
       " ['u', 'n', 'i', 'd', 'i', 'r', 'e', 'c', 't', 'i', 'o', 'n', 'a', 'l']]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[letter for letter in w] for w in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.** Filter out any non-vowals by creating the conditional statement `if letter in \"aeiou\"` within the nested list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a', 'i', 'u', 'i', 'o'],\n",
       " ['o', 'a', 'u', 'a', 'i', 'o'],\n",
       " ['e', 'o', 'u', 'i', 'o'],\n",
       " ['e', 'u', 'o', 'i', 'a'],\n",
       " ['e', 'a', 'i', 'o', 'u'],\n",
       " ['u', 'i', 'i', 'e', 'i', 'o', 'a']]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[letter for letter in w if letter in \"aeiou\"] for w in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.** Use the `''.join(` method on the nested list comprehension to convert the list of vowals into a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aiuio', 'oauaio', 'eouio', 'euoia', 'eaiou', 'uiieioa']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[''.join([letter for letter in w if letter in \"aeiou\"]) for w in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.** Convert the entire list into a set (this sorts the entries alphabetically by default):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aiuio', 'eaiou', 'eouio', 'euoia', 'oauaio', 'uiieioa'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([''.join([letter for letter in w if letter in \"aeiou\"]) for w in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 42.\n",
    "\n",
    "★ Use WordNet to create a semantic index for a text collection. Extend the concordance search program in [3.6](https://www.nltk.org/book/ch03.html#code-stemmer-indexing), indexing each word using the offset of its first synset, e.g. `wn.synsets('dog')[0].offset` (and optionally the offset of some of its ancestors in the hypernym hierarchy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 43.\n",
    "\n",
    "★ With the help of a multilingual corpus such as the Universal Declaration of Human Rights Corpus (`nltk.corpus.udhr`), and NLTK's frequency distribution and rank correlation functionality (`nltk.FreqDist`, `nltk.spearman_correlation`), develop a system that guesses the language of a previously unseen text. For simplicity, work with a single character encoding and just a few languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 44.\n",
    "\n",
    "★ Write a program that processes a text and discovers cases where a word has been used with a novel sense. For each word, compute the WordNet similarity between all synsets of the word and all synsets of the words in its context. (Note that this is a crude approach; doing it well is a difficult, open research problem.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 45.\n",
    "\n",
    "★ Read the article on normalization of non-standard words (Sproat et al, 2001), and implement a similar system for text normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
